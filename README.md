# Enhancing Large Language Model Training and Inference Using High-Performance Computing Techniques

## Description of the Project

This project aims to develop a small GPT from scratch and enhance the training and inference of the developed model using high-performance computing (HPC) techniques. Our objective is to understand and implement the GPT architecture on the Harry Potter Book dataset and improve its efficiency through various HPC methods. The ultimate goal is to create a resource-efficient model that can perform well in resource-constrained environments.

## Project Milestones

1. **Project Initialization**
   - Defined project scope and objectives.
   - Set up initial project structure and environment.

2. **Development of GPT Architecture** 
   - Developed GPT architecture from scratch.
   - Conducted extensive literature review and understanding of decoder-based models.
  
3. **Training and Evaluation**
   - Conducted multiple training iterations to optimize performance.
   - Evaluated model on the Harry Potter dataset.

3. **Profiling**
   - Implemented cProfiling to identify bottlenecks.

4. **Implementation of Data Parallelism**
   - Implemented data parallelism using PyTorch Distributed.
   - Achieved significant reduction in training time.

5. **Model Quantization**
   - Applied quantization techniques to reduce model size which reduces the Inference time.

## Model Architecture

![Copy of prog4](https://github.com/itskavyagupta/Optimized-LLM/assets/66244523/8b421f15-45cf-4c1c-834e-d3f851b83718)
